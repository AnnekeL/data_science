{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jigsaw_v13_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jl6cVGFHuHZ8",
        "gjI-VzajuHas",
        "-Qb3q4UMclyt",
        "OBQbbyTdmhGJ",
        "S7LlIB2pCWQI",
        "3Mg2mIr8mpnn",
        "tNVzqAeOnn1n",
        "e7MHMYUXntXZ",
        "FvUlKwVkuHtc",
        "yULbLAkruHyd"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87W_1p2tuHXx",
        "colab_type": "text"
      },
      "source": [
        "## Jigsaw Kaggle Competition - WIP \n",
        "\n",
        "### Proper commenting and explanation still missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnFr_i0eLkF",
        "colab_type": "text"
      },
      "source": [
        "### Link to google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnrqM8Lm2IPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl6cVGFHuHZ8",
        "colab_type": "text"
      },
      "source": [
        "#### Intro and agenda\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjI-VzajuHas",
        "colab_type": "text"
      },
      "source": [
        "### Import relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "aFeN81njuHa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import re\n",
        "sns.set()\n",
        "# To release RAM import gc collector\n",
        "import gc\n",
        "gc.enable()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVq1pAq_uHYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BHAxLHmuvzF",
        "colab_type": "code",
        "outputId": "c5101792-e0cd-4c7b-f1fc-3ca4cb8ee8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pip install emoji\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-h0XMnluHZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#workaround for zeno\n",
        "#%run 'emoji/emoji/unicode_codes.py'\n",
        "#%run 'emoji/emoji/core.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qb3q4UMclyt",
        "colab_type": "text"
      },
      "source": [
        "### Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWLkAJkFuHeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtypesDict_tr = {\n",
        "'id'                            :         'int32',\n",
        "'target'                        :         'float16',\n",
        "'severe_toxicity'               :         'float16',\n",
        "'obscene'                       :         'float16',\n",
        "'identity_attack'               :         'float16',\n",
        "'insult'                        :         'float16',\n",
        "'threat'                        :         'float16',\n",
        "'asian'                         :         'float16',\n",
        "'atheist'                       :         'float16',\n",
        "'bisexual'                      :         'float16',\n",
        "'black'                         :         'float16',\n",
        "'buddhist'                      :         'float16',\n",
        "'christian'                     :         'float16',\n",
        "'female'                        :         'float16',\n",
        "'heterosexual'                  :         'float16',\n",
        "'hindu'                         :         'float16',\n",
        "'homosexual_gay_or_lesbian'     :         'float16',\n",
        "'intellectual_or_learning_disability':    'float16',\n",
        "'jewish'                        :         'float16',\n",
        "'latino'                        :         'float16',\n",
        "'male'                          :         'float16',\n",
        "'muslim'                        :         'float16',\n",
        "'other_disability'              :         'float16',\n",
        "'other_gender'                  :         'float16',\n",
        "'other_race_or_ethnicity'       :         'float16',\n",
        "'other_religion'                :         'float16',\n",
        "'other_sexual_orientation'      :         'float16',\n",
        "'physical_disability'           :         'float16',\n",
        "'psychiatric_or_mental_illness' :         'float16',\n",
        "'transgender'                   :         'float16',\n",
        "'white'                         :         'float16',\n",
        "'publication_id'                :         'int8',\n",
        "'parent_id'                     :         'float32',\n",
        "'article_id'                    :         'int32',\n",
        "'funny'                         :         'int8',\n",
        "'wow'                           :         'int8',\n",
        "'sad'                           :         'int8',\n",
        "'likes'                         :         'int16',\n",
        "'disagree'                      :         'int16',\n",
        "'sexual_explicit'               :         'float16',\n",
        "'identity_annotator_count'      :         'int16',\n",
        "'toxicity_annotator_count'      :         'int16'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "mlhnU_DOuHeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link1 = 'https://drive.google.com/open?id=1JYgyZh1WxXglHQ--rXWtrIZImLxpkNGo'\n",
        "link3 = 'https://drive.google.com/open?id=1FB8nq7o0zn6lEcqfT_CDetYfvbmxq30a'\n",
        "\n",
        "fluff1, id1 = link1.split('=')\n",
        "fluff3, id3 = link3.split('=')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id1}) \n",
        "downloaded.GetContentFile('test.csv')  \n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id3}) \n",
        "downloaded.GetContentFile('train.csv')  \n",
        "train_data = pd.read_csv('train.csv',dtype=dtypesDict_tr,parse_dates=['created_date'], nrows=500000 )\n",
        "\n",
        "del link1, fluff1, id1, fluff3, id3, dtypesDict_tr "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUCSNE78K0p0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6c9550bc-e0f4-4451-d774-6d9ff1b79938"
      },
      "source": [
        "# kill all other columns except comment text\n",
        "cols_to_keep = ['comment_text','target']\n",
        "train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4wBNnqmuHmG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCykVT_wuHmJ",
        "colab_type": "text"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z3qWLBNuHm1",
        "colab_type": "code",
        "outputId": "737d3fc2-26b7-4b27-c8c6-a0fb9f06579d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "''' Extract all unique characters from the dataset ''' \n",
        "\n",
        "def extract_used_characters(df):\n",
        "    import string\n",
        "    used_characters = set()\n",
        "    for i, row in enumerate(df):\n",
        "        characters = list(row)\n",
        "        for x in characters:\n",
        "            used_characters.add(x)\n",
        "    used_characters = [c for c in used_characters if c not in list(string.ascii_lowercase)]\n",
        "    return used_characters"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evtlG-DYSWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove all remaining symbols\n",
        "# Some preprocesssing that will be common to all the text classification methods you will see. \n",
        "#https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The next function eliminates all signs that have been included in the previous \n",
        "defined list \"puncts\". The sign is simply replaced by a whitespace.\n",
        "\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    for x in text:\n",
        "        if x in puncts:\n",
        "            text = text.replace(x, ' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXVdeeaRQHvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Extract all unique contradictions from the dataset \n",
        "including \"-\" or \"'\" inside a word''' \n",
        "\n",
        "def extract_contractions(df):\n",
        "    contractions = set()\n",
        "    for i, row in enumerate(df):\n",
        "        words = word_tokenize(row)\n",
        "        for w in words:\n",
        "          #used_words.add(w)\n",
        "          if re.match(r\"\\b\\w*[-']\\w*\\b\", w):\n",
        "            contractions.add(w)\n",
        "                \n",
        "    return contractions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9QQy6ifQwGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This functions check's whether a word contains a contraction or not. \n",
        "In case a contraction is found, the corrected value from the dictionary is \n",
        "returned\n",
        "\"\"\"\n",
        "\n",
        "def replace_contractions(text):\n",
        "    for x in text.split():\n",
        "        if x in contraction_dict.keys():\n",
        "            text = text.replace(x, contraction_dict.get(x))\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLmSSdL-gqFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This function uses the google translator to detect comments that aren't\n",
        "english. If a comment is found it tries to detect the language and then\n",
        "translates it to english.\n",
        "\"\"\"\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "def translator(text, tlator):\n",
        "  try:\n",
        "      language = tlator.detect(text).lang\n",
        "      if language != 'en':\n",
        "          text = tlator.translate(text, dest='en')\n",
        "  except:\n",
        "      None\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kZ8os5LYrbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
        "\"\"\"\n",
        "The following function is used to format the numbers.\n",
        "\"\"\"\n",
        "\n",
        "def clean_numbers(x):\n",
        "    if bool(re.search(r'\\d+', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqiK4xuZFdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "3d5225cf-fdd7-4285-eb08-763afde0187d"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "''' Remove stopwords and multiple whitespaces around words'''\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stopword_re = re.compile(r'\\s*\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "  text = stopword_re.sub(' ', text)\n",
        "  text = re.sub(r\"\\s+\",\" \", text, flags = re.I)\n",
        "  text = re.sub(r\"^\\s+\", \"\", text)\n",
        "  text = re.sub(r\"\\s+$\", \"\", text)\n",
        "  return text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-RH82K_ZtAY",
        "colab_type": "text"
      },
      "source": [
        "##### Spellchecker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8kfng2nZy_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(df):\n",
        "  \n",
        "  '''Build a dictionary of words and its number of occurences from the data frame including all words which are not known to nlp vocabluary.\n",
        "  This filters out typical names of persons, locations etc. which are not known to common language dictionaries'''\n",
        "  \n",
        "  vocab = {}\n",
        "  for i, row in enumerate(df):\n",
        "      words = word_tokenize(row)\n",
        "      #words = [word for word in row.split(' ')] pot. faster\n",
        "      for w in words:\n",
        "        if nlp.vocab.has_vector(w):\n",
        "          pass\n",
        "        else:\n",
        "          try:\n",
        "              vocab[w] += 1\n",
        "          except KeyError:\n",
        "              vocab[w] = 1\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fYeRscrZ2wB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e6233d3d-a605-4e08-8201-ab7f859211ea"
      },
      "source": [
        "# This comes from CPMP script in the Quora questions similarity challenge. \n",
        "from collections import Counter\n",
        "import heapq\n",
        "nltk.download('punkt')\n",
        "from operator import itemgetter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word): \n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLq8nOajasgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' This method can be used to replace misspelled words using regex'''\n",
        "#Sample dict:\n",
        "#mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "  \n",
        "  ''' Compile all mispellings (keys of the dictionary) to one regex search \n",
        "  function and return the dictionary and the related regex function'''\n",
        "  \n",
        "  mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "  return mispell_dict, mispell_re\n",
        "\n",
        "\n",
        "def replace_typical_mispell(text, mispell_dict):\n",
        "  ''' Replace all mispellings in a text from the mispelling dictionary '''\n",
        "  mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "  def replace(match):\n",
        "      return mispellings[match.group(0)]\n",
        "  return mispellings_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBnfQk3YdK_",
        "colab_type": "text"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hFqtPHXPuHnB",
        "colab_type": "code",
        "outputId": "560f627a-fae0-44f8-fcc4-cd4a7c8266c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# UExtract all unique characters from the comment text as a basis for translation and/or replacement\n",
        "characters = extract_used_characters(train_data['comment_text'])\n",
        "print(characters)\n",
        "del characters\n",
        "gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['â€', 'ï¸', 'â‰', 'ğŸ®', 'â™­', 'â™', 'â€‘', 'ğŸ²', 'ÄŸ', 'ä¼š', 'ğŸ˜µ', 'Ã©', 'á´', '\\x08', 'Ø¢', 'ğŸ˜ƒ', 'ğŸ„', 'ì„¸', 'ğŸ’ƒ', 'ÊŠ', 'â€“', '×', 'Ã®', 'E', 'ğŸ‡', 'Â´', '7', 'âœ”', 'á´€', 'Øº', 'ğŸ˜”', 'á¾¶', 'ï¬', 'ğŸ‘¤', 'à¥', 'ğŸ’¨', 'æˆ¦', 'í•˜', 'æ˜¯', 'ğŸŒ¯', 'â‚ƒ', 'â—', 'ğ“¿', '\\\\', '`', 'Å', 'âœ¬', 'ÃŒ', 'Ä¡', '\\t', 'Ã·', 'Â«', 'Ğ¿', 'È™', 'Ø¹', 'ãƒ„', 'á¼´', 'Å«', 'Ù†', 'ã¤', 'S', 'A', 'åœŸ', 'äºº', 'Ã³', 'Ù', '\\xad', 'ğŸ˜ ', 'Ï„', 'Ù„', 'Éª', 'â', 'É¡', 'á´ ', 'âˆ™', 'á½¶', 'â”', 'ğŸ—‘', 'ğŸ˜‚', 'â€²', 'ğŸ‡¼', '\\u200b', 'Ã', '×›', 'Î±', 'ğŸ˜²', 'æ–‡', 'ğŸ¾', 'ğŸ˜°', 'Ïƒ', 'ğŸŒ', 'ğŸ”¤', 'ğŸº', 'R', 'à¼¼', 'Ä‡', 'ğŸ¿', 'ä½ ', 'á»‡', 'á´Š', 'â€', 'åœ†', 'ä¸', 'â€ ', 'ğŸ‘…', 'Â½', 'â–„', 'Î­', 'ğŸ¡', 'ğ˜³', 'ÊŒ', '\\u2008', 'ğŸ”', ':', '\\u200a', 'Ì', 'ğŸ»', 'â€¼', 'ğŸ’', 'å', 'ğŸ˜›', 'ğŸ’œ', 'ãƒ³', 'Ã¼', 'P', 'ğ˜´', '\\x7f', 'â˜­', '\\x95', 'â™¡', 'ğŸ•', 'á¿ƒ', 'K', 'â–’', 'á½»', 'âœ’', 'Z', 'ğŸ˜ˆ', '\\u2004', 'â™ª', '\\u2002', 'Ã¢', ';', 'â€”', '$', 'â—', 'ï¼³', 'â–±', 'â„…', 'â˜”', 'ğŸ’š', 'ğŸ¤', '8', 'Î¼', 'æœ‰', 'â€™', 'ğŸ˜¢', 'Ñ', 'Ğ–', '\\x96', 'Ñ‚', 'ğŸ™', 'Ì¶', 'D', 'Ø©', 'ğ˜®', 'ğŸ¤¡', 'â„', 'ğŸ˜Š', 'ğŸ‘„', 'è‹±', 'Ñ', 'É’', 'ğŸšŒ', 'Ã«', 'ğŸµ', 'â€¦', 'â˜†', '%', 'Â¡', 'â”—', 'ï¼°', 'Ã‰', 'Êƒ', 'Î³', 'Â¢', 'Ã¨', '\"', 'Û', 'ğŸˆ', 'á½', 'ğŸ˜£', 'ğŸ˜¬', '(', 'ğŸ’µ', 'ğŸ™€', 'Å›', 'ãƒ»', 'ğŸ‘¶', 'æ‰€', 'â€³', '\\ue613', 'Ï', 'Ä¸', 'Ï†', 'æ‹¿', '\\u200e', 'Î®', 'Å“', 'ğŸ“', 'I', 'ç¾¤', 'â„¢', 'ğŸ†', 'Ğ¶', 'ğŸ˜¤', 'ğŸ‘¿', 'á¼¸', 'Â¿', 'ğŸ‘', 'å', 'Y', 'é’±', 'ã‚¯', 'Ê€', 'âˆ', 'Øª', 'Ã‘', 'ğŸ˜†', 'ï¼¼', 'Ğ ', 'Ãª', 'â–€', 'â¥', 'Ğ±', 'â•‘', '×œ', 'ğŸŒ®', 'Ğ²', '×£', 'â…“', 'ğŸ¾', 'å›½', 'â‰ ', 'â–‘', 'ğŸ¤¢', 'Ì…', 'ğŸ…', 'ğŸ’•', 'Ä­', 'â–“', 'éšœ', 'ğŸ•', 'Ñ™', 'ğŸ‡', 'â¤µ', 'Ğ´', 'Î¿', 'ï¼‰', 'âˆ’', 'Äƒ', 'Âµ', 'Ñ†', 'â˜º', 'Â§', 'â¬…', 'Ã', 'â˜¹', 'â—•', '\\u202a', 'Â£', '×—', 'Ã¹', 'Ø´', 'ğŸ˜•', 'ğŸ™‚', 'Î·', 'ËŒ', 'ğŸ™Š', 'ğ˜§', 'Ê¼', 'Ã»', 'â—', 'ğŸ’‹', 'Ã¶', 'âŒ ', 'Ğ°', '\\n', '\\uf04a', 'É™', 'Ã€', 'ğŸŒ¹', 'á´‹', 'â–†', 'Ø³', 'ğŸ™', 'ğŸ˜§', 'ï¼²', 'â€˜', 'Ä', 'Ê²', 'â™¥', 'Ñ', 'ğŸ¶', 'ï¼¨', 'å¤±', 'ğ˜£', 'áµ‰', 'B', 'à¤•', 'ğŸ˜’', '\\x9f', 'æ®Š', 'â•', 'âœ­', 'ğŸŒŸ', 'Ğ¾', 'Ì²', 'åœ°', 'ğŸ‘‰', 'Å™', 'ğ˜µ', '2', 'á¸·', 'ğŸ‘»', 'áµ’', 'Ã²', 'Û©', '\\u202d', 'á´›', 'Í', 'ç‰¹', 'ğŸ¤£', 'ÅŒ', '\\u200f', 'á´', 'Ä“', 'ğ“µ', 'â›½', 'Î²', 'ğŸ‘²', 'Ç', 'ğŸ¤”', 'â–”', 'â–‡', '|', 'Ã­', 'ğŸ’¥', 'âœ“', 'Ø­', 'G', 'ğŸŒ¸', 'Ï‰', 'ğŸŒ', 'â™«', 'ï¼š', '×™', 'ğŸ’', 'å­', 'ğŸ™†', '^', 'åŠ ', 'Ø·', 'Ø¨', 'á½', 'Â¶', 'çš„', 'ğŸ’–', 'Ï‚', 'Ñ‰', 'ğŸ’¯', 'Ê™', 'â•ª', '×•', 'â”«', 'ğŸ˜‹', '6', '\\uf04c', 'à¤°', 'ä¸‹', '\\u2003', 'ï¼µ', 'ğŸ”—', 'â‚‚', 'Ñˆ', 'Ã¬', 'ã€‹', 'ğŸ¤¦', 'â£', 'ğŸš“', 'ã‚µ', 'æ¸©', 'ì•ˆ', 'Ùˆ', 'Ï€', '!', 'ğŸ‘', 'ğŸ˜Ÿ', 'ğŸƒ', 'éƒ½', 'ÊŸ', 'ğŸ˜³', 'T', 'X', 'ğŸ¯', 'â–°', 'æ­Œ', 'Î»', 'á¹£', '\\uf0e0', 'å…³', 'Æ°', 'Ã¡', 'Ã§', 'ğŸ™Œ', 'á½¸', 'è±†', 'â†™', 'èˆ', 'Ğ¼', '×', 'ğŸ˜€', 'ğŸ˜„', 'ğŸ’”', 'ğŸŸ', 'N', 'ğŸ¤—', 'ğŸ‘', 'ğ“’', 'O', 'Ù', 'Ã¤', 'à¼½', 'ğŸ˜­', 'â™‚', 'Ğ½', 'ğŸ–', 'Ã¦', 'â§', \"'\", '\\x9d', 'á¼”', '3', '\\uf818', 'ğŸ½', 'Ä', 'Å‚', 'Î´', 'Ñ‡', 'Ò“', 'ğŸ˜ª', 'ç„', 'Ğ', 'ã‚¨', '×¦', 'ğŸ»', 'â–¶', 'V', 'âœ‹', 'â”£', '\\u3000', 'Ã“', 'ÏŒ', 'É”', 'Ëˆ', 'è”¡', ',', 'á½²', 'â›º', 'Ä£', 'Ñ€', 'ğŸ˜º', 'Ä', 'ğŸ’¤', 'Ù‡', 'Ğ•', 'ğŸ™ƒ', 'ğŸš¬', 'Ã–', 'ìš”', 'ğŸš½', '\\x13', 'Ø¡', '5', '+', 'Ø®', 'ä¼', 'Ğ¡', 'â–ˆ', 'Å¡', '\\u2009', 'Î¯', 'Ñƒ', 'Ã´', 'æ˜', 'Ğ', 'ÍŸ', 'â€œ', 'ç¨', 'á´œ', 'á´˜', 'Ê³', 'Ğ¯', 'Â¬', 'ï¼´', 'ğŸ’°', 'Ã‡', 'ï¼¥', 'â—‡', 'É¢', 'Ë', 'ï¼©', 'ğŸ”›', 'ğŸ¼', 'â”', 'ğŸ˜‡', 'ğŸ˜Œ', 'ğŸ†', 'Â¾', 'ğŸ¶', 'É´', 'Î¹', 'á´µ', ']', 'ğŸ‘¥', 'ğŸ””', '×§', '/', 'Ùƒ', '\\u202f', 'è°·', 'ğŸ˜–', 'ğŸ‡º', 'ğŸ’ª', 'Ä«', '\\ue602', 'ğ˜¤', 'Ä¥', 'ï¬‚', '_', 'â›²', 'ğŸš„', 'ï¼¯', 'Å½', 'Ğ¸', 'Ğš', 'ğŸŒ ', 'ï¼£', 'ğŸ¤§', 'Ìˆ', '×ª', 'â–…', 'Ã¸', 'ğŸ™ˆ', 'L', 'ğŸ“š', 'ğŸš²', 'Ê', 'Ø²', '\\r', 'Ğº', 'Ä™', 'ğŸ’©', 'Îº', 'ğŸ˜', '\\ufeff', '{', 'ğŸ˜', 'á¼€', 'Ã£', 'ğŸŒ', '@', 'ğŸ˜©', 'á´·', 'Æ¡', 'Ñ•', 'å–', '×¢', '\\x81', '\\uf0b7', 'Ãœ', 'ğŸ˜œ', 'é™', 'ğŸ™„', 'Ğ', 'Ğ—', 'å…‹', 'â˜»', 'Îµ', 'ğ˜ª', 'Â°', '\\xa0', 'ğŸ˜…', 'Ğµ', '&', 'ğŸ”¥', 'ğŸ‰', 'â€º', 'Ä°', 'åˆ«', 'Âº', 'áµ˜', 'Ğ˜', 'ğŸ˜', 'Â·', 'â¥', 'ç³»', 'æ²¹', 'ğŸ˜‘', 'â´', '[', 'â—', 'Ë™', 'ğŸ™…', 'ğŸ¤´', 'ğŸ', 'ğŸŒ', 'á¼', 'Ëš', 'Ù…', 'ğŸ˜‰', 'â‚¬', 'ğ˜¦', 'Ã', '\\x9c', 'Ø±', 'ğŸ‘', 'Ë¢', 'Ğ³', 'ğŸ˜¨', 'Å„', 'Ğ·', 'ã‚¹', 'ÑŒ', 'ğŸ°', 'W', 'Q', 'ğŸ˜¥', 'ğŸ°', 'Ğœ', 'ğ˜°', 'Ñ…', ')', '4', 'ğŸ˜¯', 'ğŸ‘ ', 'Ï', 'â€', '.', 'ÙŠ', 'Ø£', 'ğŸ’€', 'Ğ”', '\\u200d', '×‘', 'ğŸ‘Š', '9', 'ğŸ‘Œ', ' ', 'ğŸ’›', '-', 'á´„', 'Ğ¤', 'Ù‚', 'ğŸ¤‘', 'ğŸŒº', 'â€¢', 'â†’', 'ğŸ˜±', '\\u2028', 'å¤–', 'ğŸ˜', 'Ù', '\\x80', 'Êœ', 'M', 'â–¬', 'ï¼', 'ï¼…', '×', '?', 'Ã±', 'ğŸ‘', 'É›', 'âœ¨', 'Ğ¹', 'ğŸ˜“', 'ØŸ', 'ã‚¸', 'ğŸ˜', 'á´‡', 'å°', 'æˆ', 'ï¼­', 'âº', 'ğŸ˜', 'Ê¸', 'Ä—', '~', 'ğŸµ', 'Ê»', 'à¤®', '0', 'ğŸ’—', 'Ê¿', 'è®®', 'ğŸ', 'F', 'C', 'ğŸ’™', 'Ê°', 'ğ˜¢', 'U', 'ğŸ€', 'ğŸš€', 'ğŸ‘®', '1', 'ğŸ‘€', 'â–‚', '\\x85', 'â‚„', 'ğŸ˜¡', 'â¡', 'ğŸ±', 'Ñ›', 'ğŸ†•', '× ', 'â—„', 'ğŸ˜', 'â—¾', 'ä½œ', 'ğ˜º', 'É‘', '\\ue014', 'æ•…', 'â„', 'Î¾', 'Ù‰', '\\u2005', 'ğŸ¤¥', 'à®œ', 'â–ƒ', 'Ê·', 'â€›', 'ğŸ™‰', 'Å', 'Â¯', '>', 'á´º', 'ğ˜Š', 'Ì„', '×”', 'ğŸ†', 'Ã', 'ğŸ˜˜', 'â¤', '\\u202c', '×š', 'Â®', 'â€•', 'ĞŸ', 'à¤¾', 'å»', 'ãƒ¬', 'Øµ', 'á´…', '×˜', 'ï¼', 'ì˜', 'Ãˆ', 'Â¹', 'J', 'Ã¯', 'â–º', 'Í', 'Â»', 'â“', 'Ñ‹', '×¨', 'ğŸ“£', 'éª—', 'â‚', 'ğŸ¤“', '\\x10', 'Â©', 'å›­', 'â˜ ', '=', 'H', 'ğŸ’«', 'Â¥', '×©', 'ğŸ˜´', 'Â±', 'Å˜', '<', 'Ã„', 'Î¬', 'Â²', 'Ø¥', 'Ï…', '×“', 'Ø¯', 'áµ—', 'Ä€', 'â¤', 'â˜•', 'Î½', 'â‰ˆ', 'å¤§', 'Ã ', 'Ğ»', '\\x92', 'ğŸ–‘', 'ğŸ‡¸', 'Ğ’', 'â‹…', 'â­', 'Â¼', 'â–·', 'Ñ', 'ğ“²', 'Ø§', 'â‡’', 'ï¿¼', 'ğŸ‘†', 'Ãº', 'ğŸ˜®', 'ÃŸ', 'ğŸ‡¹', '*', 'â˜…', '}', 'å“¥', 'á´¡', 'âœ', 'ğŸ¾', 'á´¼', 'âœ˜', 'ğŸ”„', '#', 'ğŸ½', 'ã€‚']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avunqB2WdV-F",
        "colab_type": "text"
      },
      "source": [
        "The extraction of all characters shows that a wide list of symbols, foreign and emojies are being used. As a major part of them representing a certain meaning.\n",
        "The following steps will step by step translate the symbols into meaningful words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC9nR5XKe4ty",
        "colab_type": "text"
      },
      "source": [
        "Firstly, emojies are translated into words using an out of the box library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5kDcW-kuHoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the emoji library for translation\n",
        "import emoji\n",
        "# Translate the emojies into common words\n",
        "train_data.comment_text = train_data.comment_text.apply(lambda x: emoji.demojize(x))\n",
        "del emoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCBeeekrffMs",
        "colab_type": "text"
      },
      "source": [
        "Based on the EDA we saw that most of the comments are in English. However, the closer inspection of the symbols shows that the dataset contains Chinese or Arabic letters. We will thus translate the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvQy_K_dgdpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiate the translator\n",
        "transl = Translator()\n",
        "# Translate the comments which are not in english\n",
        "train_data.comment_text = train_data.comment_text.apply(lambda x: translator(x, transl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LcPb9OAuHoJ",
        "colab_type": "code",
        "outputId": "b9902f25-b9cf-4bd4-ca7f-52060f0582ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# UExtract all unique characters from the comment text as a basis for translation and/or replacement\n",
        "other_characters = extract_used_characters(train_data['comment_text'])\n",
        "other_characters.remove(' ')\n",
        "print(other_characters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "] Â¹ Ï„ ^ = T á½ Éª á´˜ ğ˜¤ á¾¶ çš„ Â§ â–€ ì•ˆ â—„ & Ø® á´· Ñ áµ˜ Ã¹ Î» ÍŸ âœ“ ã‚¯ ğ“µ Ñƒ # ×“ â–± \u0013 Â¶ â™­ Â¬ è±† Ø¢ â†’ á´› Ê¿ E Ğ¹ æ²¹ Â¥ Ä¥ Ï… Ù† â„ É¢ Ì„ á½¶ Î¯ äºº Ø² A âœ­ Ø´ Ëˆ É´ á¸· Â« + â–’ à¤° ÊŠ ğ˜ª â€˜ C [ â‰ˆ â€• á´¼ Ù… ×© á¿ƒ è‹± Ğ´ à¥ â€³ ä½œ Ê€ Û 5 ×  S ×• Ã³ Ä­ æ˜ ï¼‰ â¥ Ù å…‹ âœ˜ Ï å¤– Îº â€ƒ Øº ~ ğ˜´ Ä Ì‡ ä¸‹ } Ã« Øµ Ã­ á´ Ø¡ ï¼… Î· âˆ™ à®œ ï½• Â¼ â€­ ğ˜£ áµ‰ > ğ˜§ ğ˜¢ Å¡ å¤§ Ã± â–· Ã§ Ã® á´„ @ ×› ( N Î¾ å– Ê™ Ä‘ - á½¸ ' â¥ ) ã‚¹ á´€ â˜» ×— Ëš ì˜ Ñ€ Ğ° ÄŸ Ã¯ ã‚¸ å›½ Ã¸ _ æ‰€ â—‡ Äƒ Ï Ë Ê Â ğ“² * â€› Ç 0 ï½‰ Ã© Å« éƒ½ Ä— Ã· Â¿ ×” Ã  â— ç„ Ø¨ ğ˜Š î˜‚ Ãº ã€‚ Ã¡ × ï¼ Ñ‚ Â ï¬ âŒ  \r ? : É¡ Â° â‚¬ ç¾¤ ×¨ Ê¸ Â» Ø§ ä¼š ØŸ á´º â–„ ğ“’ Ğ³ 1 Â• Ñ æ®Š Ê² É‘ \n",
            " Ã¨ á´… Ï€ ğ˜° ï½’ Ù„ Ø¯ æ­Œ Ê³ É› ä½  â˜† â™¡ Ñ‹ ï»¿ Ò“ â€‹ åœŸ â€  Î± á¼” ×§ ! × â‚ƒ Ğ¾ å 7 æ¸© Ì… åŠ  Ñ• â˜­ Ä‡ æ˜¯ Ğº æ•… â‚‚ Â€ â–“ Ã¶ × ğ˜® K Å‚ á´  ï¬‚ Ñ… â–° Å“ â€™ ÃŸ O à¼¼ Ğ» Îµ Â¾ î€” ÙŠ æˆ¦ åœ† Ø± Å¾ Î­ ï½ æ–‡ ï½ å›­ í•˜ Ãª Ä¡ á¼ â€ª â–‡ 6 Ø³ $ Å â€œ â€ ç‰¹ á½ ` 3 ; Ê» â§ á¹£ Ù \t Ì Ğ² Å„ Ë™ Ã¬ ï½ à¼½ å­ á´‹ â€¨ â„… ï¼¼ ğ˜µ â™« ãƒ„ Î¿ Å™ ï½ƒ Ï† Ã¦ Ê¼ Ñ™ Ù‰ Û© \" Êœ å Î¼ ï¼ ïŠ ãƒ» Â­ Ø£ é™ Ùƒ â€ âœ¬ â–‚ Â… Ğ½ â–† â„ ×¢ Å ÊŒ ãƒ¬ % â–… Ã» Î® ì„¸ â€² à¤® Í ğŸ–‘ Ñ› ×‘ Â´ , / Ã£ \\ á´ ïƒ  2 Â¡ Î´ Â¢ U . Ì² ã‚µ Î² Ñ â‚„ é’± â€Š \b â”£ â€‰ â´ â¤ Ğ¿ ç³» â€… â˜… â€“ ã€‹ ï½“ ï¼š 4 ï ˜ â–º Ê· Â² â‹… â™ª å¤± Ğ¸ Ì¶ ğ“¿ è®® É’ ä¸ â€¯ á´œ ×™ â€ â•ª ËŒ áµ’ â–” âˆ â€¦ â€¢ Ä« âˆ’ ïŒ Í á´‡ éª— É” ìš” Ã¤ â–‘ Â½ â–ƒ Âº æœ‰ Ê° Î½ Î¬ â• Êƒ â–ˆ â‰  Âµ Â  á»‡ ÏŒ èˆ ï½” ×ª Â¯ ï‚· á´µ åœ° Ğµ â€º Øª W â€„ ï¿¼ å…³ è°· Â’ î˜“ Î³ Ï‚ Ù‚ æˆ 8 ï½ˆ ğ˜º á½² ×š Ù‡ Ã² ×£ Âœ å° ×˜ Ğ· åˆ« Ä“ å» Â± â— â‚ â” ×œ â–¬ Ã´ Ø© Ã¼ É™ | â€ è”¡ V Ø· â—• Î¹ Ñ† { â…“ Ñ„ â”« â€” á´Š Ñˆ ã€€ Ñ‡ â€‚ Ë¢ Ñ Ø¹ ä¼ Ä™ Ø­ Ä£ Â· ÊŸ Ğ¶ Ñ‰ ğ˜¦ Æ° Å› Ğ± ï½… å“¥ ÂŸ à¤• â— â•‘ Ä¸ ğ˜³ Ğ¼ â€‘ ã‚¨ â€ Æ¡ Â£ á´¡ Ï‰ Ùˆ â€¬ áµ— æ‹¿ ÑŒ Ã¢ á¼´ ãƒ³ Ìˆ Ä 9 Ïƒ â€ˆ ã¤ á¼€ Â– á¼° à¤¾ È™ ç¨  Ù â”— á½» éšœ \u0010 Ø¥ < ×¦ â‡’ "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5sYB8YL4V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract contradictions from the dataset\n",
        "contradictions = extract_contractions(train_data['comment_text'])\n",
        "del contradictions\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Ql1WxAj5ri",
        "colab_type": "text"
      },
      "source": [
        "##### Spellchecking with Spacy and GoogleNews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW4uesYTtXmd",
        "colab_type": "code",
        "outputId": "8302ba32-2f0e-4c54-8477-0d21815730f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# pip install spacy\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Build the vocabluary and delete the big datasets afterwards\n",
        "vocab = build_vocab(train_data['comment_text'])\n",
        "del nlp, spacy\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "328"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNkssj7Tz0bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load GoogleNews vocabluary to the disk\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcDD1y3KuHrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Assign the embedding Word2Vec from Google News and load the model\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \n",
        "                                                        binary=True)\n",
        "del EMBEDDING_FILE\n",
        "\n",
        "# Extract the words from the model\n",
        "words = model.index2word\n",
        "\n",
        "# Rank the words of the vocabluary\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "\n",
        "WORDS = w_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IiV-RcsuHsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select the top occuring words from unknown dataset\n",
        "top_words = dict(heapq.nlargest(1000, vocab.items(), key=itemgetter(1)))\n",
        "print(top_words)\n",
        "del vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj7YmCy_uHsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map the word correction to the top mispelled words\n",
        "pool = Pool(4)\n",
        "corrected_words = pool.map(correction,list(top_words.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UASPyBDfuHsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate the missell dict\n",
        "mispell_dict = {}\n",
        "for word,corrected_word in zip(top_words,corrected_words):\n",
        "    if word!=corrected_word:\n",
        "        mispell_dict[word] = corrected_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXpVNDaCVYH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show the library\n",
        "mispell_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0AvLz2kMknz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace mispellings according to the dictionary\n",
        "train_data.comment_text = train_data.comment_text.apply(lambda x: replace_typical_misspell(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhf6sxUorKo7",
        "colab_type": "code",
        "outputId": "88905f9c-6102-4e1d-bdc4-9b001af78ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Delete nun-needed libraries\n",
        "del mispell_dict, WORDS, words, w_rank, model, pool,Pool, corrected_words, itemgetter\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZEk3T7sYbo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%whos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpVcXoxmmHyq",
        "colab_type": "text"
      },
      "source": [
        "### Visualize most common words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXu0iDBuuHp2",
        "colab_type": "text"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "stop_words = stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnfI079TuHp8",
        "colab_type": "text"
      },
      "source": [
        "-# Tokenize the comment text\n",
        "tok_comments = [word_tokenize(com) for com in train_data.comment_text]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFf-AW6buHqF",
        "colab_type": "text"
      },
      "source": [
        "-# Remove stopwords\n",
        "tokens = [[w for w in s if (w not in stop_words) & (len(w)>2)] for s in tok_comments]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTiFWj-uHqM",
        "colab_type": "text"
      },
      "source": [
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blvD5npNuHqn",
        "colab_type": "text"
      },
      "source": [
        "-#plot the most frequent words\n",
        "tokens = np.array([np.array(s) for s in tokens])\n",
        "fdist = FreqDist(np.concatenate(tokens))\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Azz_at1uHqz",
        "colab_type": "text"
      },
      "source": [
        "-#plot the most frequent word pairs\n",
        "from nltk import bigrams, ngrams\n",
        "bigrams_tokens = bigrams(np.concatenate(tokens))\n",
        "fdist_bigrams = FreqDist(list(bigrams_tokens))\n",
        "fdist_bigrams.plot(30,cumulative=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBhQF02uHtI",
        "colab_type": "text"
      },
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrYju3MguHtK",
        "colab_type": "text"
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnKwn5hhuHtM",
        "colab_type": "text"
      },
      "source": [
        "dictionary = Dictionary(tok_comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OcPgm-MuHtU",
        "colab_type": "text"
      },
      "source": [
        "dictionary.token2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6HUuE5uHta",
        "colab_type": "text"
      },
      "source": [
        "corpus = [dictionary.doc2bow(com) for com in tok_comments]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_57wPVTcXl2",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvM7hlPlaVR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Make use of the build in Text Blob sensitivity features to add them towards a model\n",
        "train_data['polarity'] = train_data['comment_text'].apply(lambda com: TextBlob(com).sentiment.polarity)\n",
        "train_data['subjectivity'] = train_data['comment_text'].apply(lambda com: TextBlob(com).sentiment.subjectivity)\n",
        "del TextBlob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQbbyTdmhGJ",
        "colab_type": "text"
      },
      "source": [
        "## Count vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpxeU7jtmoWc",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9PCJi6uHue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsUJpPTuHun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate the target variable into a binary output variable to fit to logistic regression and other clasification models\n",
        "y = np.where(train_data.target>0.5,1, 0 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0ws42nOuHuw",
        "colab_type": "text"
      },
      "source": [
        "Using count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK5_rZTpuHuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiate the count vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english') #, max_features=1000\n",
        "# Create a count_vectorizer for out dataset\n",
        "count_data = count_vectorizer.fit_transform(train_data.comment_text.values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_XVSgZ4I9-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#count_data_arr = count_data.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbuaS7Tx-a6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#count_data_df = pd.DataFrame(count_data_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWyzxPp50Y3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del count_vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05_bhOU4y4B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add_features = ['polarity', 'subjectivity']\n",
        "#count_data = pd.concat([count_data_df,train_data.loc[:, add_features]], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipeHvoMRCzPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do we need scaling??\n",
        "\n",
        "# Feature Scaling\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "#sc = StandardScaler()\n",
        "#count_data = sc.fit_transform(count_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaXfCT1NuHu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do a train-test-split on the counted data\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(count_data, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6nkqfDynPLc",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhRccOdLuHwI",
        "colab_type": "code",
        "outputId": "56526c6b-86df-43bf-f7c7-f8e26ed01785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "# Fit a first logictic regression model\n",
        "log = LogisticRegression()\n",
        "log.fit(X_train_c, y_train_c)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epvsFf8OuHwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on the text_set\n",
        "pred = log.predict(X_test_c)\n",
        "pred_prob = log.predict_proba(X_test_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJdmU_4SuHwh",
        "colab_type": "code",
        "outputId": "e747615d-a9ca-46fe-85be-a5b379fc9045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Get the accuracy\n",
        "metrics.accuracy_score(y_test_c, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XeAWWsMuHw5",
        "colab_type": "code",
        "outputId": "5dde8799-a5c1-4347-d9c5-e5c1d19388a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Get the ROC AUC score\n",
        "metrics.roc_auc_score(y_test_c, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7337503728242796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7LlIB2pCWQI",
        "colab_type": "text"
      },
      "source": [
        "### Light GBM  - to be adapted to non binary Y variable, and loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxL4UG0LCbls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
        "import lightgbm as lgb\n",
        "\n",
        "d_train = lgb.Dataset(x_train, label=y_train)\n",
        "\n",
        "params = {}\n",
        "params['learning_rate'] = 0.003\n",
        "params['boosting_type'] = 'gbdt'\n",
        "params['objective'] = 'binary' # ADAPT\n",
        "params['metric'] = 'binary_logloss' # ADAPT\n",
        "params['sub_feature'] = 0.5\n",
        "params['num_leaves'] = 10\n",
        "params['min_data'] = 50\n",
        "params['max_depth'] = 10\n",
        "\n",
        "clf = lgb.train(params, d_train, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2r5Ec0Clbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prediction\n",
        "y_pred=clf.predict(x_test)\n",
        "\n",
        "#convert into binary values\n",
        "\n",
        "for i in range(0,99):\n",
        "    if y_pred[i]>=.5:       # setting threshold to .5\n",
        "       y_pred[i]=1\n",
        "    else:  \n",
        "       y_pred[i]=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Sum1rADf3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "#Accuracy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_pred,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IUjT550DwSY",
        "colab_type": "text"
      },
      "source": [
        "Parameter Tuning:\n",
        "\n",
        "Data scientists always struggle in deciding when to use which parameter? and what should be the ideal value of that parameter?\n",
        "\n",
        "Following set of practices can be used to improve your model efficiency.\n",
        "\n",
        "    num_leaves: This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting.\n",
        "    min_data_in_leaf: Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n",
        "    max_depth: You also can use max_depth to limit the tree depth explicitly.\n",
        "\n",
        "For Faster Speed:\n",
        "\n",
        "    Use bagging by setting bagging_fraction and bagging_freq\n",
        "    Use feature sub-sampling by setting feature_fraction\n",
        "    Use small max_bin\n",
        "    Use save_binary to speed up data loading in future learning\n",
        "    Use parallel learning, refer to parallel learning guide.\n",
        "\n",
        "For better accuracy:\n",
        "\n",
        "    Use large max_bin (may be slower)\n",
        "    Use small learning_rate with large num_iterations\n",
        "    Use large num_leaves(may cause over-fitting)\n",
        "    Use bigger training data\n",
        "    Try dart\n",
        "    Try to use categorical feature directly\n",
        "\n",
        "To deal with over-fitting:\n",
        "\n",
        "    Use small max_bin\n",
        "    Use small num_leaves\n",
        "    Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
        "    Use bagging by set bagging_fraction and bagging_freq\n",
        "    Use feature sub-sampling by set feature_fraction\n",
        "    Use bigger training data\n",
        "    Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
        "    Try max_depth to avoid growing deep tree\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "I implemented LightGBM on multiple datasets and found that its accuracy challenged other boosting algorithms. From my experience, I will always recommend you to try this algorithm at least once.\n",
        "\n",
        "I hope you guys enjoyed this blog and it was useful to all. I would request you you to give suggestion which will help to improve this blog.\n",
        "\n",
        "Source: Microsoft LightGBM Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mg2mIr8mpnn",
        "colab_type": "text"
      },
      "source": [
        "## TFIDF vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNVzqAeOnn1n",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2FM9SMPuHxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_data = tfidf_vectorizer.fit_transform(train_data.comment_text.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQe_dHKz3orK",
        "colab_type": "code",
        "outputId": "e4dcb2c7-2883-4e99-f677-dc334dae8d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tfidf_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<500000x129727 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10555981 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiiA9vcIuHxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(tfidf_data, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7MHMYUXntXZ",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnvWJkZuHxW",
        "colab_type": "code",
        "outputId": "34b6929a-021b-46b0-c292-46b6a769f257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "log = LogisticRegression()\n",
        "log.fit(X_train_t, y_train_t)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtK6n016uHyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = log.predict(X_test_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D26jOu0uHyK",
        "colab_type": "code",
        "outputId": "3b8d3fea-29f6-488c-82d5-bb2058369902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "metrics.accuracy_score(y_test_t, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4oLI8ZduHyW",
        "colab_type": "code",
        "outputId": "ed473835-ca6b-4e04-fdf3-2b0a378c5bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "metrics.roc_auc_score(y_test_t, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7040973935998638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ef_XewouHtG",
        "colab_type": "text"
      },
      "source": [
        "#### !! Use spacy to maybe correct only non-names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvUlKwVkuHtc",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTBwnbEnWzvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0q9nJCCs-sE",
        "colab_type": "code",
        "outputId": "fc4a25c2-ec3e-439e-a7d4-191a3fb77177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "\n",
        "# Tokenize the train data based on spaces\n",
        "tokenized_corpus = [[w for w in document.split(' ')] for document in train_data['comment_text']]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cool',\n",
              "  'like',\n",
              "  'would',\n",
              "  'want',\n",
              "  'mother',\n",
              "  'read',\n",
              "  'really',\n",
              "  'great',\n",
              "  'idea',\n",
              "  'well',\n",
              "  'done',\n",
              "  ''],\n",
              " ['thank',\n",
              "  'would',\n",
              "  'make',\n",
              "  'life',\n",
              "  'lot',\n",
              "  'le',\n",
              "  'anxiety',\n",
              "  'inducing',\n",
              "  'keep',\n",
              "  'let',\n",
              "  'anyone',\n",
              "  'get',\n",
              "  'way',\n",
              "  ''],\n",
              " ['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive', ''],\n",
              " ['something', 'able', 'install', 'site', 'releasing', ''],\n",
              " ['haha', 'guy', 'bunch', 'loser', '']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j7S5WCTtFXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set values for various parameters\n",
        "feature_size = 100    # Word vector dimensionality  \n",
        "window_context = 5          # Context window size                                                                                    \n",
        "min_word_count = 1   # Minimum word count                        \n",
        "sample = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "#Create a word2vec model\n",
        "w2v_model = Word2Vec(tokenized_corpus, size=feature_size, \n",
        "                              window=window_context, min_count = min_word_count,\n",
        "                              sample=sample, iter=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmuMR27XtITz",
        "colab_type": "text"
      },
      "source": [
        "####### visualize embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "words = w2v_model.wv.index2word\n",
        "wvs = w2v_model.wv[words]\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "  plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHDu3UOAuJ7h",
        "colab_type": "code",
        "outputId": "a5451078-14a0-4c0c-c5a9-1d0f7412cafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "''' Average the word vectors to get a vector per document '''\n",
        "\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "   \n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "# get document level embeddings\n",
        "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
        "                                             num_features=feature_size)\n",
        "#pd.DataFrame(w2v_feature_array)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XJZvDtnxZ1Oz",
        "colab": {}
      },
      "source": [
        "X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(w2v_feature_array, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17yzsHfvZ1QX"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wzyq0fiYZ1Qm",
        "outputId": "9bc59fe2-d20f-456f-9ac9-a13623bcff66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "log = LogisticRegression()\n",
        "log.fit(X_train_t, y_train_t)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yshDsHlEZ1R5",
        "colab": {}
      },
      "source": [
        "pred = log.predict(X_test_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6lnn2Ep8Z1Sc",
        "outputId": "ac8a0b35-013b-4c09-9edf-53bc72607410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "metrics.accuracy_score(y_test_t, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PbIEVEGDZ1TT",
        "outputId": "0196da3f-b920-486c-f7b8-95c479c7b63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "metrics.roc_auc_score(y_test_t, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7040973935998638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yULbLAkruHyd",
        "colab_type": "text"
      },
      "source": [
        "##### So far model quality very bad comparing to the proportion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeBMoeptuHyh",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "    \n",
        "    Build a neural network with embeddings, using Glove, \n",
        "    in the last dense leyer we should use sigmoid function!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiEnRVEOuHyj",
        "colab_type": "text"
      },
      "source": [
        "Try also LightGBM, Naive Bayes, CNN, RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i8oMyKlw-vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}